{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6f31f8-055b-4d42-914c-5addf6171a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02192c3-2ea8-4b88-afd9-1d2d36fe615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data.utils import makedirs, save_info, load_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80212d56-d33e-4c61-a7e6-d3c778a3d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal.datasets import KwaiTagRecoDataset\n",
    "from multimodal.core import (mean_average_precision, mean_class_accuracy,\n",
    "                    mmit_mean_average_precision, top_k_accuracy,\n",
    "                    top_k_recall, top_k_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293563d3-ebff-492b-999a-e252c83d42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KwaiNodeCollator():\n",
    "    def __init__(self, g, block_sampler, test_mode):\n",
    "        self.g = g\n",
    "        self.block_sampler = block_sampler\n",
    "        self.test_mode = test_mode\n",
    "        self.pos_etype = ('tag', 'HasVideo', 'video')\n",
    "        self.neg_etype = ('tag', 'NotHasVideo', 'video')\n",
    "        self.infer_etype = ('tag', 'WhetherHasVideo', 'video')\n",
    "    \n",
    "    def collate(self, video_nids):\n",
    "        if self.test_mode:\n",
    "            return self.collate_infer(video_nids)\n",
    "        return self.collate_train(video_nids)\n",
    "    \n",
    "    def collate_train(self, video_nids):\n",
    "        items = {'video': torch.tensor(video_nids, dtype=self.g.idtype)}\n",
    "        # Sample pos & neg graph\n",
    "        pos_pair_eids = self.g.in_edges(items['video'], form='eid', etype=self.pos_etype)\n",
    "        neg_pair_eids = self.g.in_edges(items['video'], form='eid', etype=self.neg_etype)\n",
    "        pair_graph = self.g.edge_subgraph({self.pos_etype: pos_pair_eids, \n",
    "                                           self.neg_etype: neg_pair_eids})\n",
    "        # no need to apply transform.compact_graphs() since there can't be isolated nodes\n",
    "        # Sample MFGs\n",
    "        seed_nodes = pair_graph.ndata[dgl.NID]\n",
    "        blocks = self.block_sampler.sample_blocks(self.g, seed_nodes)\n",
    "        input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "        pair_graph.ndata.pop('feat')\n",
    "        batch = {\n",
    "            'input_nodes': input_nodes, \n",
    "            'pair_graph': pair_graph, \n",
    "            'mfgs': blocks\n",
    "        }\n",
    "        from dgl import dataloading\n",
    "        return batch\n",
    "    \n",
    "    def collate_infer(self, video_nids):\n",
    "        items = {'video': torch.tensor(video_nids, dtype=self.g.idtype)}\n",
    "        # Sample pos & neg graph\n",
    "        unknown_pair_eids = self.g.in_edges(items['video'], form='eid', etype=self.infer_etype)\n",
    "        unknown_pair_graph = self.g.edge_subgraph({self.infer_etype: unknown_pair_eids})\n",
    "        unknown_pair_graph.ndata.pop('feat')\n",
    "        # no need to apply transform.compact_graphs() since there can't be isolated nodes\n",
    "        # Sample MFGs\n",
    "        seed_nodes = unknown_pair_graph.ndata[dgl.NID]\n",
    "        blocks = self.block_sampler.sample_blocks(self.g, seed_nodes)\n",
    "        input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "        batch = {\n",
    "            'input_nodes': input_nodes, \n",
    "            'pair_graph': unknown_pair_graph, \n",
    "            'mfgs': blocks\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930f1124-a6bd-46d7-96c3-ee013f06362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trace real video and tags\n",
    "# new_video_nid = 8\n",
    "# new_tag_nids = torch.where(labels[new_video_nid]!=0)[0]\n",
    "# video_nid = pair_graph.ndata[dgl.NID]['video'][new_video_nid].item()\n",
    "# tag_nids = pair_graph.ndata[dgl.NID]['tag'][new_tag_nids].tolist()\n",
    "# print(video_nid2pid[video_nid])\n",
    "# for tag_nid in tag_nids:\n",
    "#     print(tag_nid2tag[tag_nid])\n",
    "#\n",
    "# original_tagid = pair_graph.ndata[dgl.NID]['tag']\n",
    "# original_videoid = pair_graph.ndata[dgl.NID]['video']\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59036c-b552-43d1-866f-224d2e25d7f2",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adbfc26-4710-4142-8411-5bf278b54ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical = 'cosmetic'\n",
    "split = 'train'\n",
    "dataset_root = '/home/wangxiao13/annotation/data/kwai'\n",
    "video_emb_dir = 'video_feat'\n",
    "tag_emb_dir = 'tag_feat_bert'\n",
    "\n",
    "num_gnn_layers = 2\n",
    "\n",
    "tag_nid2tag_path = osp.join(dataset_root, f'cache/{vertical}_{split}_tag_nid2tag.pkl')\n",
    "video_nid2pid_path = osp.join(dataset_root, f'cache/{vertical}_{split}_video_nid2pid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40465ae0-7f4e-4e1a-b6d1-ccbe26e7840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fanouts_base = [{\n",
    "    ('tag', 'HasVideo', 'video'): 0,\n",
    "    ('tag', 'NotHasVideo', 'video'): 0,\n",
    "    ('video', 'FollowedBy', 'video'): 0,\n",
    "    ('tag', 'SubTopic', 'tag'): 8,\n",
    "    ('video', 'HasTag', 'tag'): 8}]\n",
    "train_fanouts_final = [{\n",
    "    ('tag', 'HasVideo', 'video'): 0,\n",
    "    ('tag', 'NotHasVideo', 'video'): 0,\n",
    "    ('video', 'FollowedBy', 'video'): 0,\n",
    "    ('tag', 'SubTopic', 'tag'): 8,\n",
    "    ('video', 'HasTag', 'tag'): 8}]\n",
    "infer_fanouts_base = [{\n",
    "    ('tag', 'WhetherHasVideo', 'video'): 0,\n",
    "    ('tag', 'HasVideo', 'video'): 0,\n",
    "    ('tag', 'NotHasVideo', 'video'): 0,\n",
    "    ('video', 'FollowedBy', 'video'): 0,\n",
    "    ('tag', 'SubTopic', 'tag'): 8,\n",
    "    ('video', 'HasTag', 'tag'): 8}]\n",
    "infer_fanouts_final = [{\n",
    "    ('tag', 'WhetherHasVideo', 'video'): 0,\n",
    "    ('tag', 'HasVideo', 'video'): 0,\n",
    "    ('tag', 'NotHasVideo', 'video'): 0,\n",
    "    ('video', 'FollowedBy', 'video'): 0,\n",
    "    ('tag', 'SubTopic', 'tag'): 8,\n",
    "    ('video', 'HasTag', 'tag'): 8}]\n",
    "train_fanouts = train_fanouts_base*(num_gnn_layers-1) + train_fanouts_final\n",
    "infer_fanouts = infer_fanouts_base*(num_gnn_layers-1) + infer_fanouts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0b0383b-951a-4b18-b989-6783ff183dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset ...\n"
     ]
    }
   ],
   "source": [
    "dataset = KwaiTagRecoDataset(vertical=vertical,\n",
    "                             split=split,\n",
    "                             dataset_root=dataset_root,\n",
    "                             video_emb_dir=video_emb_dir,\n",
    "                             tag_emb_dir=tag_emb_dir,\n",
    "                             force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9489dc9-b504-4b03-873a-0e78962f2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset.g\n",
    "tag_nid2tag = load_info(tag_nid2tag_path)\n",
    "video_nid2pid = load_info(video_nid2pid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e2ba481-7ae3-47e0-bbc7-593642ab7ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'tag': 3114, 'video': 77539},\n",
       "      num_edges={('tag', 'HasVideo', 'video'): 308665, ('tag', 'NotHasVideo', 'video'): 241147781, ('tag', 'SubTopic', 'tag'): 6955, ('video', 'FollowedBy', 'video'): 6241272, ('video', 'HasTag', 'tag'): 308665},\n",
       "      metagraph=[('tag', 'video', 'HasVideo'), ('tag', 'video', 'NotHasVideo'), ('tag', 'tag', 'SubTopic'), ('video', 'video', 'FollowedBy'), ('video', 'tag', 'HasTag')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba02f231-c1ef-4c99-9782-d0ee6ed7e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_videos = g.num_nodes('video')\n",
    "try:\n",
    "    test_videos = g.num_edges(etype=('tag', 'WhetherHasVideo', 'video')) // g.num_nodes('tag')\n",
    "    train_videos = total_videos - test_videos\n",
    "except:\n",
    "    train_videos = 0\n",
    "\n",
    "test_mode = False if split == 'train' else True\n",
    "if test_mode:\n",
    "    block_sampler = dgl.dataloading.MultiLayerNeighborSampler(infer_fanouts)\n",
    "else:\n",
    "    block_sampler = dgl.dataloading.MultiLayerNeighborSampler(train_fanouts)\n",
    "collator = KwaiNodeCollator(g, block_sampler, test_mode=test_mode)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    list(range(train_videos, total_videos)),\n",
    "    batch_size=1024,\n",
    "    collate_fn=collator.collate,\n",
    "    shuffle=not test_mode,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ad94be1-501e-4897-9ade-b266f249960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "input_nodes, pair_graph, mfgs = list(example_minibatch.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e05fa08-e1b3-4f33-b00c-1cff829e36e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "    SRC: 24798 videos, 3114 tags\n",
      "    DST: 17945 videos, 3114 tags\n",
      "Layer 2:\n",
      "    SRC: 17945 videos, 3114 tags\n",
      "    DST: 1024 videos, 3114 tags\n"
     ]
    }
   ],
   "source": [
    "for idx, mfg in enumerate(mfgs):\n",
    "    print('Layer %d:' % (idx+1))\n",
    "    print('    SRC: %d videos, %d tags' % (mfg.num_src_nodes('video'), mfg.num_src_nodes('tag')))\n",
    "    print('    DST: %d videos, %d tags' % (mfg.num_dst_nodes('video'), mfg.num_dst_nodes('tag')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3179dcc-0942-412e-8b68-57b54a3cb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FollowedBy, SubTopic, HasTag\n",
    "Layer 1:\n",
    "    SRC: 37371 videos, 3114 tags\n",
    "    DST: 20686 videos, 3114 tags\n",
    "Layer 2:\n",
    "    SRC: 20686 videos, 3114 tags\n",
    "    DST: 1024 videos, 3114 tags\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa739113-3bd9-4050-b31a-76eb35579e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
